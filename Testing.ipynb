{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from utils import (cosine_similarity, get_dict, process_tweet)\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/Datasets/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of the English to French Training Dictionary is: 5000\n"
     ]
    }
   ],
   "source": [
    "# Loading the English to French Dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print(f'The Length of the English to French Training Dictionary is: {len(en_fr_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of the English to French Test Dictionary is: 1500\n"
     ]
    }
   ],
   "source": [
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print(f'The Length of the English to French Test Dictionary is: {len(en_fr_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "english_set = set(en_embeddings.vocab)\n",
    "french_set = set(fr_embeddings.vocab)\n",
    "en_embeddings_subset = {}\n",
    "fr_embeddings_subset = {}\n",
    "\n",
    "french_words = set(en_fr_train.values())\n",
    "\n",
    "for en_word in en_fr_train.keys():\n",
    "    fr_word = en_fr_train[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "pickle.dump(en_embeddings_subset, open(\"en_embeddings.p\", \"wb\"))\n",
    "pickle.dump(en_embeddings_subset, open(\"fr_embeddings.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of the English to French Training Dictionary is: 5000\n",
      "The Length of the English to French Training Dictionary is: 1500\n"
     ]
    }
   ],
   "source": [
    "# Loading the English to French Dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print(f\"The Length of the English to French Training Dictionary is: {len(en_fr_train)}\")\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print(f\"The Length of the English to French Training Dictionary is: {len(en_fr_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    \"\"\"\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "    \n",
    "    # Get the English Words\n",
    "    english_set = english_vecs.keys()\n",
    "    \n",
    "    # Get the French Words\n",
    "    french_set = french_vecs.keys()\n",
    "    \n",
    "    # Store the French Words that are part of the English-French Dictionary\n",
    "    french_words = set(en_fr.values())\n",
    "    \n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "    X = np.vstack(X_l)\n",
    "    Y = np.vstack(Y_l)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train,fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4932, 300), (4932, 300))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    diff = np.dot(X,R) - Y\n",
    "    \n",
    "    diff_squared = diff ** 2\n",
    "    \n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    \n",
    "    loss = sum_diff_squared / m\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    gradient = np.dot(X.transpose(), np.dot(X,R) - Y) * (2/m)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    np.random.seed(129)\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    for i in range(train_steps):\n",
    "        if i%25 ==0 :\n",
    "            print(f\"Loss at Iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        \n",
    "        R -= learning_rate * gradient\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 is: 3.7242\n",
      "Loss at Iteration 25 is: 3.6283\n",
      "Loss at Iteration 50 is: 3.5350\n",
      "Loss at Iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(129)\n",
    "\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 is: 963.0146\n",
      "Loss at Iteration 25 is: 97.8292\n",
      "Loss at Iteration 50 is: 26.8329\n",
      "Loss at Iteration 75 is: 9.7893\n",
      "Loss at Iteration 100 is: 4.3776\n",
      "Loss at Iteration 125 is: 2.3281\n",
      "Loss at Iteration 150 is: 1.4480\n",
      "Loss at Iteration 175 is: 1.0338\n",
      "Loss at Iteration 200 is: 0.8251\n",
      "Loss at Iteration 225 is: 0.7145\n",
      "Loss at Iteration 250 is: 0.6534\n",
      "Loss at Iteration 275 is: 0.6185\n",
      "Loss at Iteration 300 is: 0.5981\n",
      "Loss at Iteration 325 is: 0.5858\n",
      "Loss at Iteration 350 is: 0.5782\n",
      "Loss at Iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(v, candidates, k=1):\n",
    "    similarity_l = []\n",
    "    \n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "        \n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    k_idx = sorted_ids[-k:]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    pred = np.dot(X,R)\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbour(pred[i], Y)\n",
    "        \n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set is: 0.557\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on Test Set is: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
